{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5366df",
   "metadata": {},
   "source": [
    "# ANU ASTR4004 Week 9\n",
    "\n",
    "Author: Dr Sven Buder (sven.buder@anu.edu.au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ccb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#initialize a random number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041bee4",
   "metadata": {},
   "source": [
    "## Model Selection and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53231740",
   "metadata": {},
   "source": [
    "You have data, you've estimated and re-estimated your uncertainties using the empirical methods described above, and now you want to fit a line to it. But is a line the right thing? Maybe there are two separate relations and some break point? Surely a curve would be better? Maybe there are two different physical models that could describe what you're seeing?\n",
    "\n",
    "Model selection is a complicated topic, of which I'm only going to scratch the surface in order to highlight that such methods exist, provide some examples, and demonstrate a few simple implementations of them in practice. More information can be found in, for example, Ding et al. 2019 (https://arxiv.org/abs/1810.09583), and a miriad of additional tools for model selection can be found in https://scikit-learn.org/.\n",
    "\n",
    "In many cases, the _very first question_ you should be asking yourself is if there is a physical reason to adopt a particular model vs. another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's generate some data\n",
    "N_points = 30 #number of points\n",
    "noise_scale = 0.1 #scaling for noise\n",
    "true_order = 3\n",
    "poly_coeffs = rng.uniform(low=-1, high=1, size=true_order+1)\n",
    "#poly_coeffs = [0.5, -0.33, 0.137, 0.5] #some random polynomial coefficients\n",
    "\n",
    "x = rng.uniform(size=N_points)*2 - 1.\n",
    "y = np.polynomial.polynomial.polyval(x, poly_coeffs)\n",
    "yerr = 0.05 + noise_scale*rng.random(N_points)\n",
    "y += yerr*rng.standard_normal(size=N_points)\n",
    "\n",
    "#plot the polynomial and data\n",
    "fig, ax = plt.subplots()\n",
    "rx = np.linspace(-1,1,100)\n",
    "ax.plot(rx, np.polynomial.polynomial.polyval(rx, poly_coeffs), 'k-', label='fiducial model')\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"o\", color='0.7', label='mock data')\n",
    "ax.legend()\n",
    "plt.xlabel('x value')\n",
    "plt.ylabel('y value')\n",
    "plt.title('Mock data drawn from polynomial')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0171f35",
   "metadata": {},
   "source": [
    "### AIC and BIC\n",
    "Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC) are two different ways to compare models of varying complexity, accounting for both changes in likelihood and penalising the inclusion of additional parameters (more complex models). \n",
    "\n",
    "For a given model, the AIC is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{AIC} = 2k - 2\\ln\\left(\\hat{L}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is the number of free parameters in the model and $\\hat{L}$ is the maximum value of the likelihood function. BIC has a similar form but with a different penalty for the number of parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{BIC} = k\\ln(n) - 2\\ln\\left(\\hat{L}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "The additional parameter $n$ is the number of data points used to constrain the model.\n",
    "\n",
    "The different forms of AIC and BIC come from different assumptions with regard to priors in their formulation, and they have fundamentally different goals. Selection via AIC aims to minimise loss for predictive purposes, while BIC aims to identify the best model for inferrence purposes. In general the smaller penalty for complex models in AIC tends to favour a larger number of parameters.\n",
    "\n",
    "Suffice it to say there's a lot to unpack between these two that is beyond the scope of this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at how BIC and AIC compare for a simple case of polynomial fitting here\n",
    "orders = np.arange(1,10)\n",
    "\n",
    "fig, ax = plt.subplots(len(orders), figsize=(8,2*len(orders)), sharex=True)\n",
    "rx = np.linspace(-1,1,100)\n",
    "\n",
    "store_aic = np.zeros(len(orders))\n",
    "store_bic = np.zeros(len(orders))\n",
    "\n",
    "ylow, yhigh = y.min()-0.05*y.ptp(), y.max()+0.5*y.ptp()\n",
    "\n",
    "for ii, polyorder in enumerate(orders):\n",
    "\n",
    "    #estimate best-fit coefficients for this polynomial order\n",
    "    coeffs = np.polynomial.polynomial.polyfit(x, y, polyorder, w=1./yerr)\n",
    "\n",
    "    #estimate log(L) from best fit, assuming Gaussian uncertainties\n",
    "    diff = (y-np.polynomial.polynomial.polyval(x, coeffs))**2\n",
    "    sigma2 = yerr**2\n",
    "    logl = -0.5*np.sum(diff/sigma2 + np.log(sigma2))\n",
    "\n",
    "    #compute AIC and BIC\n",
    "    aic = 2*(polyorder+1) - 2*logl\n",
    "    bic = (polyorder+1)*np.log(len(x)) - 2*logl\n",
    "    \n",
    "    #save AIC/BIC for plotting later\n",
    "    store_aic[ii] = aic\n",
    "    store_bic[ii] = bic\n",
    "    \n",
    "    #plot panels showing data and best fit in ax\n",
    "    ax[ii].errorbar(x,y,yerr=yerr, fmt='o', color='0.7', ms=3)\n",
    "    ax[ii].plot(rx, np.polynomial.polynomial.polyval(rx, coeffs), 'k-')\n",
    "    \n",
    "    ax[ii].axis([-1,1,0.1,0.9])\n",
    "    \n",
    "    ax[ii].text(0.02, 0.05, f'Polynomial Order: {polyorder}; AIC: {aic:.0f}; BIC: {bic:.0f}', \n",
    "                size=12, transform=ax[ii].transAxes)\n",
    "\n",
    "    ax[ii].set_ylabel('y value')\n",
    "\n",
    "    ax[ii].axis([-1,1,ylow, yhigh])\n",
    "\n",
    "ax[-1].set_xlabel('x value')\n",
    "plt.show()\n",
    "\n",
    "#plot showing AIC/BIC vs. polyorder\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(orders, store_aic, '-', color='C0', label='AIC')\n",
    "ax2.plot(orders, store_bic, '-.', color='C1', label='BIC')\n",
    "\n",
    "#get some ranges for the axes\n",
    "aymin = np.minimum(store_aic.min(), store_bic.min())\n",
    "aymax = np.maximum(store_aic.max(), store_bic.max())\n",
    "ax2.plot([true_order,true_order],[aymin-5,aymax+5], 'r-', zorder=0.9)\n",
    "\n",
    "ax2.legend()\n",
    "plt.ylim(aymin-5, aymax+5)\n",
    "ax2.set_xlabel('polynomial order')\n",
    "ax2.set_ylabel('AIC/BIC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c864d3b",
   "metadata": {},
   "source": [
    "### Leave-one-out cross validation (LOOCV)\n",
    "\n",
    "Cross validation as a general tool involves splitting the sample into \"training\" and \"testing\" subsamples. The model is then fitted or trained using the training subsample, and its performance then evaluated by comparing predictions with the testing sample. In this way, it is considered an out-of-sample test (for comparison, an example of an in-sample leave-one-out test is jacknifing). Its behaviour is asymptotically equivalent to AIC (in the large sample limit), however it involves fewer assumptions about how additional parameters should be penalised. CV in general is a common approach in the context of machine learning, and so it's worth being familiar with the idea even if you're not likely to use it much.\n",
    "\n",
    "LOOCV involves iterating the modelling procedure over the sample data, each time excluding one point, and using that to evaluate the out-of-sample performance. At each iteration one stores the mean square error for the left-out sample, and the final metric is the mean of those mean square error values.\n",
    "\n",
    "LOOCV is a particular limit of *k*-fold cross validation, which involves dividing the data into *k* different subsets for training/testing, with *k* equal to the number of samples. For larger samples, LOOCV can be inefficient and adopting *k*-fold validation with *k*=5 or 10 can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0ce16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's take our polynomial example again\n",
    "orders = np.arange(1,6)\n",
    "\n",
    "store_mse = np.zeros(len(orders))\n",
    "\n",
    "# Let's do leave-one-out cross-validation for each polynomial order\n",
    "for ii, polyorder in enumerate(orders):\n",
    "    mean_off = 0.\n",
    "\n",
    "    # Not let's leave one point out at a time at position jj\n",
    "    for jj in range(len(x)):\n",
    "        coeffs = np.polynomial.polynomial.polyfit(np.r_[x[:jj],x[jj+1:]],\n",
    "                                                  np.r_[y[:jj],y[jj+1:]],\n",
    "                                                  polyorder)\n",
    "        #use the fitted coefficients to measure the offset for the excluded point\n",
    "        mean_off += ((y[jj]-np.polynomial.polynomial.polyval(x[jj], coeffs))/yerr[jj])**2\n",
    "    store_mse[ii] = mean_off / (len(x)-1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(orders, store_mse)\n",
    "\n",
    "ax.axvline(true_order, color='r')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_xlabel('Polynomial order')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1361a",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856501a",
   "metadata": {},
   "source": [
    "We'll generate some sample time series data for the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sample time series data\n",
    "np.random.seed(42)\n",
    "time_index = pd.date_range(start='2020-01-01', periods=100, freq='M')\n",
    "data = pd.Series(np.random.randn(100).cumsum() + 50, index=time_index)\n",
    "\n",
    "# Plot the time series data\n",
    "plt.plot(data, label='Original Data')\n",
    "plt.title('Sample Time Series Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf3f6e",
   "metadata": {},
   "source": [
    "### Simple Exponential Smoothing (SES)\n",
    "\n",
    "Simple Exponential Smoothing is a time series forecasting method for univariate data without any trend or seasonality. It only requires one smoothing parameter (α)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258116d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Exponential Smoothing\n",
    "ses_model = SimpleExpSmoothing(data).fit(smoothing_level=0.2, optimized=False)\n",
    "ses_forecast = ses_model.forecast(12)\n",
    "\n",
    "# Plot the original data and the SES forecast\n",
    "plt.plot(data, label='Original Data')\n",
    "plt.plot(ses_forecast, label='SES Forecast', color='red')\n",
    "plt.title('Simple Exponential Smoothing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249fde4",
   "metadata": {},
   "source": [
    "In this example, we've set the smoothing parameter (α) to 0.2. The fit method trains the model, and forecast is used to generate future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635e183",
   "metadata": {},
   "source": [
    "### Holt's Linear Method\n",
    "\n",
    "Holt's Linear Method extends SES to capture both level and trend. It is useful when the data exhibits a linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b32996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt's Linear Method\n",
    "holt_model = ExponentialSmoothing(data, trend='add').fit()\n",
    "holt_forecast = holt_model.forecast(12)\n",
    "\n",
    "# Plot the original data and the Holt's Linear forecast\n",
    "plt.plot(data, label='Original Data')\n",
    "plt.plot(holt_forecast, label=\"Holt's Linear Forecast\", color='red')\n",
    "plt.title(\"Holt's Linear Method\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23c6d6",
   "metadata": {},
   "source": [
    "Here, the trend='add' option tells the model to include a linear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcbda6",
   "metadata": {},
   "source": [
    "### Additive Damped Trend Method\n",
    "\n",
    "The additive damped trend method is a variation of Holt’s linear method where the trend is damped over time, meaning that the trend decreases in impact as the forecast extends into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Damped Trend Method\n",
    "damped_model = ExponentialSmoothing(data, trend='add', damped_trend=True).fit()\n",
    "damped_forecast = damped_model.forecast(12)\n",
    "\n",
    "# Plot the original data and the damped trend forecast\n",
    "plt.plot(data, label='Original Data')\n",
    "plt.plot(damped_forecast, label=\"Additive Damped Trend Forecast\", color='red')\n",
    "plt.title(\"Additive Damped Trend Method\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b20adb",
   "metadata": {},
   "source": [
    "The damped_trend=True option applies damping to the trend component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895ca45",
   "metadata": {},
   "source": [
    "### Holt-Winters Method (Triple Exponential Smoothing)\n",
    "\n",
    "The Holt-Winters method adds a seasonal component to Holt's linear method. It is especially useful for data with both a trend and seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt-Winters Method\n",
    "hw_model = ExponentialSmoothing(data, trend='add', seasonal='add', seasonal_periods=12).fit()\n",
    "hw_forecast = hw_model.forecast(12)\n",
    "\n",
    "# Plot the original data and the Holt-Winters forecast\n",
    "plt.plot(data, label='Original Data')\n",
    "plt.plot(hw_forecast, label=\"Holt-Winters Forecast\", color='red')\n",
    "plt.title(\"Holt-Winters (Triple Exponential Smoothing)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdce452",
   "metadata": {},
   "source": [
    "The seasonal='add' option introduces seasonality, and seasonal_periods=12 assumes a yearly seasonal pattern (since our data is monthly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa4906",
   "metadata": {},
   "source": [
    "### In Practice?\n",
    "\n",
    "Let's look at some time-series data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10010779",
   "metadata": {},
   "source": [
    "#### SES for ASX200\n",
    "\n",
    "The ASX A200 index represents a wide range of industries in Australia, and the index’s movements can be influenced by various market conditions such as economic policies, global events, and company earnings.\n",
    "\n",
    "Given the random noise and short-term fluctuations, **Simple Exponential Smoothing (SES)** is a helpful tool for identifying the long-term level of the index by smoothing out day-to-day variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84770d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)   # ValueWarning subclasses UserWarning\n",
    "\n",
    "start_training = '2020-01-01'\n",
    "end_training = '2024-09-15'\n",
    "\n",
    "# start_predicting = '2024-09-01'\n",
    "# end_predicting = '2024-10-01'\n",
    "\n",
    "start_predicting = '2024-09-16'\n",
    "end_predicting = '2025-10-01'\n",
    "\n",
    "# Fetch ASX A200 index data (ASX:A200 ticker)\n",
    "asx_a200 = yf.download('^AXJO', start=start_training, end=end_training, auto_adjust=True)\n",
    "asx_a200_actual = yf.download('^AXJO', start=start_predicting, end=end_predicting, auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asx_a200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to predict the Close Price\n",
    "data_train = asx_a200['Close']\n",
    "data_test = asx_a200_actual['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast the same number of days as in data_test\n",
    "forecast_length = len(data_test)\n",
    "forecast_index = data_test.index\n",
    "\n",
    "# Let's try Simple Exponential Smoothing first\n",
    "ses_model = SimpleExpSmoothing(data_train).fit(smoothing_level=0.2, optimized=False)\n",
    "ses_forecast = ses_model.forecast(forecast_length)\n",
    "ses_forecast.index = data_test.index#pd.date_range(start=start_predicting, periods=forecast_length, freq='B')\n",
    "\n",
    "# Apply Holt's ExponentialSmoothing Linear Trend method\n",
    "holt_model = ExponentialSmoothing(data_train, trend='add').fit()\n",
    "holt_forecast = holt_model.forecast(forecast_length)\n",
    "holt_forecast.index = forecast_index\n",
    "\n",
    "hw_model = ExponentialSmoothing(data_train, trend='add', seasonal='add', seasonal_periods=5*12).fit()\n",
    "hw_forecast_seasonal = hw_model.forecast(forecast_length)\n",
    "hw_forecast_seasonal.index = forecast_index\n",
    "\n",
    "# Plot the stock data, SES forecast, and actual data\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(data_train, label='ASX A200 Index Price', color = 'k')\n",
    "plt.plot(ses_forecast, label='SES Forecast', color='C2')\n",
    "plt.plot(holt_forecast, label=\"Holt's ExponentialSmoothing Linear Forecast\", color='C6')\n",
    "plt.plot(hw_forecast_seasonal, label=\"Holt-Winters Seasonal Forecast (12 weeks)\", color='C4')\n",
    "plt.plot(data_test, label='ASX A200 Index Price Actual', color='C3')\n",
    "plt.xlim(pd.Timestamp(start_training), pd.Timestamp('2025-10-01'))\n",
    "plt.title('Simple Exponential Smoothing on ASX A200 Index (2020 to 2025)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d5113",
   "metadata": {},
   "source": [
    "#### Holt's Linear Trend Method on Australian Retail Sales**\n",
    "\n",
    "Retail sales often exhibit both a level and a trend over time, reflecting growth or decline in the economy. Holt’s Linear Trend method can be applied to model both the level and trend of retail sales data.\n",
    "\n",
    "You can download Australian retail sales data from the Australian Bureau of Statistics (ABS) or from public datasets like Kaggle.\n",
    "\n",
    "For this tutorial, I have gone to the ABS website (https://www.abs.gov.au) and searched for \"Retail Trade, Australia\" and then downloaded Table 1 of \"Retail turnover, by industry group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e351cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_sales_abs = pd.read_excel('data/abs_retail_sales.xlsx',1,skiprows=9)\n",
    "retail_sales_abs['date'] = pd.to_datetime(retail_sales_abs['Series ID'])\n",
    "retail_sales_abs.set_index('date', inplace=True)\n",
    "\n",
    "# let's only look at the last 100 months\n",
    "retail_sales_abs = retail_sales_abs[-100:]\n",
    "\n",
    "# Let's only look at Food Retailing\n",
    "retail_sales_column = 'A3348591K' # Turnover ;  Total (State) ;  Food retailing ;\n",
    "retail_sales_name = 'Food retailing'\n",
    "\n",
    "retail_sales_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "# Apply Simple Exponential Smoothing (SES)\n",
    "ses_model = SimpleExpSmoothing(retail_sales_abs[retail_sales_column]).fit(smoothing_level=0.2, optimized=False)\n",
    "ses_forecast = ses_model.forecast(12)  # Forecasting 12 months ahead\n",
    "\n",
    "# Plot the original data and the SES forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(retail_sales_abs[retail_sales_column], label=retail_sales_name)\n",
    "plt.plot(ses_forecast, label='SES Forecast', color='red')\n",
    "plt.title('Simple Exponential Smoothing on ABS Retail Sales Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf04abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Apply Holt's Linear Trend method\n",
    "holt_model = ExponentialSmoothing(retail_sales_abs[retail_sales_column], trend='add').fit()\n",
    "holt_forecast = holt_model.forecast(12)  # Forecasting 12 months ahead\n",
    "\n",
    "# Plot the original data and Holt's Linear forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(retail_sales_abs[retail_sales_column], label=retail_sales_name)\n",
    "plt.plot(holt_forecast, label=\"Holt's Linear Forecast\", color='red')\n",
    "plt.title(\"Holt's Linear Trend Method on ABS Retail Sales Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Additive Damped Trend method\n",
    "damped_model = ExponentialSmoothing(retail_sales_abs[retail_sales_column], trend='add', damped_trend=True).fit()\n",
    "damped_forecast = damped_model.forecast(12)\n",
    "\n",
    "# Plot the original data and the damped trend forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(retail_sales_abs[retail_sales_column], label=retail_sales_name)\n",
    "plt.plot(damped_forecast, label=\"Additive Damped Trend Forecast\", color='red')\n",
    "plt.title(\"Additive Damped Trend Method on ABS Retail Sales Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Holt-Winters (Triple Exponential Smoothing)\n",
    "hw_model = ExponentialSmoothing(retail_sales_abs[retail_sales_column], trend='add', seasonal='add', seasonal_periods=12).fit()\n",
    "hw_forecast = hw_model.forecast(12)\n",
    "\n",
    "# Plot the original data and the Holt-Winters forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(retail_sales_abs[retail_sales_column], label=retail_sales_name)\n",
    "plt.plot(hw_forecast, label=\"Holt-Winters Forecast\", color='red')\n",
    "plt.title(\"Holt-Winters Method on ABS Retail Sales Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f4f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
